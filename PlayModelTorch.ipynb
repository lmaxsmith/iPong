{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9da3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blah\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print('blah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b970ed6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "24937e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_softmax_stack = nn.Sequential(\n",
    "            nn.Linear(8, 6),\n",
    "            nn.LogSoftmax(),\n",
    "            nn.Linear(6, 4),\n",
    "            nn.LogSoftmax(),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_softmax_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "763a141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved state\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_softmax_stack): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=6, bias=True)\n",
      "    (1): LogSoftmax(dim=None)\n",
      "    (2): Linear(in_features=6, out_features=4, bias=True)\n",
      "    (3): LogSoftmax(dim=None)\n",
      "    (4): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model_path = 'my_model.pt'\n",
    "\n",
    "if(os.path.exists(model_path)):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Loaded saved state\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9ab79f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All returns: tensor([[-0.4271]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "Predicted class: tensor([0], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 8, device=device)\n",
    "logits = model(X)\n",
    "print(f\"All returns: {logits}\")\n",
    "\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a9e2d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "play_data = pd.read_table('TrainingQueue.tsv')\n",
    "\n",
    "\n",
    "X = play_data.drop(columns=['MyPaddleVelocity','Unnamed: 9']).dropna()\n",
    "y = play_data['MyPaddleVelocity'].dropna() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5ac9195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MyPaddlePosition  MyPaddleTimeAsIs  OpponentPaddle  OpponentVelocity  \\\n",
      "0                -1.000          1.384546        -2.00000               0.0   \n",
      "1                -1.000          1.414189        -2.00000               0.0   \n",
      "2                -1.000          1.439879        -2.00000               0.0   \n",
      "3                -1.000          1.462690        -2.00000               0.0   \n",
      "4                -1.000          1.487308        -2.00000               0.0   \n",
      "...                 ...               ...             ...               ...   \n",
      "15496             0.245          2.207550         1.70413               0.0   \n",
      "15497             0.245          2.227112         1.70413               0.0   \n",
      "15498             0.245          2.246399         1.70413               0.0   \n",
      "15499             0.245          2.267944         1.70413               0.0   \n",
      "15500             0.245          2.289093         1.70413               0.0   \n",
      "\n",
      "       BallPositionX  BallPositionY  BallVelocityX  BallVelocityY  \n",
      "0           0.000000      -0.300000       0.000000        -3.0000  \n",
      "1           0.000000      -0.360000       0.000000        -3.0000  \n",
      "2           0.000000      -0.420000       0.000000        -3.0000  \n",
      "3           0.000000      -0.540000       0.000000        -3.0000  \n",
      "4           0.000000      -0.600000       0.000000        -3.0000  \n",
      "...              ...            ...            ...            ...  \n",
      "15496      -1.738816       0.465812      -1.152462        -3.3075  \n",
      "15497      -1.761865       0.399662      -1.152462        -3.3075  \n",
      "15498      -1.784914       0.333512      -1.152462        -3.3075  \n",
      "15499      -1.807963       0.267362      -1.152462        -3.3075  \n",
      "15500      -1.831012       0.201212      -1.152462        -3.3075  \n",
      "\n",
      "[15501 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "class PongDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X.values\n",
    "        self.labels = y.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return feature, label\n",
    "\n",
    "play_data = pd.read_table('TrainingQueue.tsv')\n",
    "X = play_data.drop(columns=['MyPaddleVelocity', 'Unnamed: 9']).dropna()\n",
    "y = play_data['MyPaddleVelocity'].dropna()\n",
    "\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6d1cbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for the training and validation sets\n",
    "train_dataset = PongDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = PongDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2f6630ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/30], Train Loss: 0.5842, Val Loss: 4.3005\n",
      "Epoch [1/30], Train Loss: 1.6620, Val Loss: 4.2108\n",
      "Epoch [2/30], Train Loss: 0.2995, Val Loss: 3.7702\n",
      "Epoch [3/30], Train Loss: 2.5758, Val Loss: 3.0740\n",
      "Epoch [4/30], Train Loss: 0.2742, Val Loss: 2.9900\n",
      "Epoch [5/30], Train Loss: 1.5837, Val Loss: 2.8679\n",
      "Epoch [6/30], Train Loss: 1.1724, Val Loss: 2.8375\n",
      "Epoch [7/30], Train Loss: 0.9759, Val Loss: 2.8634\n",
      "Epoch [8/30], Train Loss: 1.2868, Val Loss: 2.8619\n",
      "Epoch [9/30], Train Loss: 2.4291, Val Loss: 2.9601\n",
      "Epoch [10/30], Train Loss: 1.8179, Val Loss: 2.9209\n",
      "Epoch [11/30], Train Loss: 0.2827, Val Loss: 2.9678\n",
      "Epoch [12/30], Train Loss: 2.3782, Val Loss: 2.9691\n",
      "Epoch [13/30], Train Loss: 1.7127, Val Loss: 2.9275\n",
      "Epoch [14/30], Train Loss: 0.5883, Val Loss: 2.8750\n",
      "Epoch [15/30], Train Loss: 1.4983, Val Loss: 2.9776\n",
      "Epoch [16/30], Train Loss: 1.6749, Val Loss: 2.9373\n",
      "Epoch [17/30], Train Loss: 3.0150, Val Loss: 2.8921\n",
      "Epoch [18/30], Train Loss: 1.3110, Val Loss: 2.9116\n",
      "Epoch [19/30], Train Loss: 2.5964, Val Loss: 2.9013\n",
      "Epoch [20/30], Train Loss: 2.6182, Val Loss: 2.9971\n",
      "Epoch [21/30], Train Loss: 1.0804, Val Loss: 2.9774\n",
      "Epoch [22/30], Train Loss: 1.4706, Val Loss: 2.9924\n",
      "Epoch [23/30], Train Loss: 2.1912, Val Loss: 2.8914\n",
      "Epoch [24/30], Train Loss: 2.3955, Val Loss: 2.9181\n",
      "Epoch [25/30], Train Loss: 0.4837, Val Loss: 2.8730\n",
      "Epoch [26/30], Train Loss: 3.2366, Val Loss: 2.9152\n",
      "Epoch [27/30], Train Loss: 2.3435, Val Loss: 2.9155\n",
      "Epoch [28/30], Train Loss: 0.4174, Val Loss: 2.9169\n",
      "Epoch [29/30], Train Loss: 0.8428, Val Loss: 2.9410\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = .001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Move the batch of training data to the device\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch = X_batch.float()\n",
    "        y_batch = y_batch.float()\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move the batch of validation data to the device\n",
    "            X_batch, y_batch = batch\n",
    "            X_batch = X_batch.float()\n",
    "            y_batch = y_batch.float()\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            val_loss = loss_fn(y_pred, y_batch.unsqueeze(1))\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d6be7441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.1138, Accuracy: 66.17%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on the test set\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        # Move the batch of testing data to the device\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch = X_batch.float()\n",
    "        y_batch = y_batch.float()\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        test_loss += loss_fn(y_pred, y_batch).item()\n",
    "\n",
    "        # Compute number of correct predictions\n",
    "        correct += (y_pred.squeeze().round() == y_batch.round()).type(torch.float).sum().item()\n",
    "\n",
    "\n",
    "test_loss /= len(val_loader.dataset)\n",
    "accuracy = correct / len(val_loader.dataset)\n",
    "\n",
    "print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ca37b94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu),\n",
      "      %linear_softmax_stack.0.weight : Float(6, 8, strides=[8, 1], requires_grad=1, device=cpu),\n",
      "      %linear_softmax_stack.0.bias : Float(6, strides=[1], requires_grad=1, device=cpu),\n",
      "      %linear_softmax_stack.2.weight : Float(4, 6, strides=[6, 1], requires_grad=1, device=cpu),\n",
      "      %linear_softmax_stack.2.bias : Float(4, strides=[1], requires_grad=1, device=cpu),\n",
      "      %linear_softmax_stack.4.weight : Float(1, 4, strides=[4, 1], requires_grad=1, device=cpu),\n",
      "      %linear_softmax_stack.4.bias : Float(1, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/flatten/Flatten_output_0 : Float(1, 8, strides=[8, 1], requires_grad=0, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/flatten/Flatten\"](%input), scope: __main__.NeuralNetwork::/torch.nn.modules.flatten.Flatten::flatten # /Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/flatten.py:46:0\n",
      "  %/linear_softmax_stack/linear_softmax_stack.0/Gemm_output_0 : Float(1, 6, strides=[6, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/linear_softmax_stack/linear_softmax_stack.0/Gemm\"](%/flatten/Flatten_output_0, %linear_softmax_stack.0.weight, %linear_softmax_stack.0.bias), scope: __main__.NeuralNetwork::/torch.nn.modules.container.Sequential::linear_softmax_stack/torch.nn.modules.linear.Linear::linear_softmax_stack.0 # /Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/linear_softmax_stack/linear_softmax_stack.1/LogSoftmax_output_0 : Float(1, 6, strides=[6, 1], requires_grad=1, device=cpu) = onnx::LogSoftmax[axis=1, onnx_name=\"/linear_softmax_stack/linear_softmax_stack.1/LogSoftmax\"](%/linear_softmax_stack/linear_softmax_stack.0/Gemm_output_0), scope: __main__.NeuralNetwork::/torch.nn.modules.container.Sequential::linear_softmax_stack/torch.nn.modules.activation.LogSoftmax::linear_softmax_stack.1 # /Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1930:0\n",
      "  %/linear_softmax_stack/linear_softmax_stack.2/Gemm_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/linear_softmax_stack/linear_softmax_stack.2/Gemm\"](%/linear_softmax_stack/linear_softmax_stack.1/LogSoftmax_output_0, %linear_softmax_stack.2.weight, %linear_softmax_stack.2.bias), scope: __main__.NeuralNetwork::/torch.nn.modules.container.Sequential::linear_softmax_stack/torch.nn.modules.linear.Linear::linear_softmax_stack.2 # /Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/linear_softmax_stack/linear_softmax_stack.3/LogSoftmax_output_0 : Float(1, 4, strides=[4, 1], requires_grad=1, device=cpu) = onnx::LogSoftmax[axis=1, onnx_name=\"/linear_softmax_stack/linear_softmax_stack.3/LogSoftmax\"](%/linear_softmax_stack/linear_softmax_stack.2/Gemm_output_0), scope: __main__.NeuralNetwork::/torch.nn.modules.container.Sequential::linear_softmax_stack/torch.nn.modules.activation.LogSoftmax::linear_softmax_stack.3 # /Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1930:0\n",
      "  %output : Float(1, 1, strides=[1, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/linear_softmax_stack/linear_softmax_stack.4/Gemm\"](%/linear_softmax_stack/linear_softmax_stack.3/LogSoftmax_output_0, %linear_softmax_stack.4.weight, %linear_softmax_stack.4.bias), scope: __main__.NeuralNetwork::/torch.nn.modules.container.Sequential::linear_softmax_stack/torch.nn.modules.linear.Linear::linear_softmax_stack.4 # /Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%output)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/logan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Export model\n",
    "model.to('cpu')\n",
    "onnx_file_name = \"torch_model.onnx\"\n",
    "dummy_input = torch.randn(1,8)\n",
    "\n",
    "# Export the trained model to ONNX format\n",
    "torch.onnx.export(model,                 # model being run\n",
    "                  dummy_input,           # dummy input tensor\n",
    "                  onnx_file_name,        # output file name\n",
    "                  verbose=True,          # print out a lot of information\n",
    "                  input_names=[\"input\"], # specify the name of the input\n",
    "                  output_names=[\"output\"], # specify the name of the output\n",
    "                  opset_version=10)      # the ONNX version to use\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854c701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0942d097a3d85f7f7e6db728cda1faec53ae278c49874135db056ee1e5e1978"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
